%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%                                                                       %%
%%     LaTeX + CTeX 《应用概率统计》论文模板, 只针对 A4 纸中文稿.        %%
%%                                                                       %%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%            中文稿 文章模板：A4 纸, 五号字, 单列              %
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\documentclass[a4paper,c1size,onecolumn,twoside,cap,Chinese]{APSart}
\usepackage{listings} 
\usepackage{enumitem}
\usepackage{algorithm}
\usepackage{amsmath} 
\usepackage{algpseudocode}
\graphicspath{{figs/}}
\floatname{algorithm}{Class} 
\usepackage{hyperref}
\begin{document}

\setcounter{page}{1}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%------------------ 编辑部提供的信息 ------------------------%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\newcommand{\pubvol}{xx}         % 卷号
\newcommand{\enpubvol}{xx}       % 卷号
\newcommand{\pubno}{x}           % 期号
\newcommand{\enpubno}{x}         % 期号
\newcommand{\pubyear}{20xx}      % 出版年份
\newcommand{\enpubyear}{20xx}    % 出版年份
\newcommand{\pubmonth}{xx}       % 出版月份
\newcommand{\enpubmonth}{xx}     % 出版月份
\newcommand{\ksym}{xxx}          % 开始页码
\newcommand{\jsym}{xxx}          % 结束页码
\newcommand{\receivedate}{本文XXXX年XX月XX日收到} % 论文收到日期
\newcommand{\modifydate}{XXXX年XX月XX日收到修改稿}% 论文修改日期
\newcommand{\doino}{10.3969/j.issn.1001-4268.20xx.0x.0xx} % doi号
%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%-------------------- 作者提供的信息 ------------------------%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\newcommand{\runcnauthors}{~} %超过两个作者的请用：第一作者姓名~等
\newcommand{\cnfirstauthor}{陈越琦}
\newcommand{\cnsecondauthor}{刘威}
\newcommand{\cnthirdauthor}{杨杰才}
\newcommand{\cnfourthauthor}{周子博}
\newcommand{\cnfirstinst}{121160005 ~~Yueqichen.0x0@gmail.com}
\newcommand{\cnsecondinst}{131220085 ~~liuwei13cs@smail.nju.edu.cn}
\newcommand{\cnthirdinst}{131220115 ~~mark\_grove@qq.com}
\newcommand{\cnfourthinst}{121250229 ~~441842096@qq.com}
\newcommand{\cntitle}{大数据实验3---HBase\_Hive~实验报告}
\newcommand{\cnkeywords}{Hadoop、倒排索引、HBase、Hive}
\newcommand{\cnclassno}{O212.xx} % 中图分类号
%%
%\newcommand{\enfirstauthor}{FIRST Name}
%\newcommand{\ensecondauthor}{SECOND Name}
%\newcommand{\enfirstinst}{First Author's Working Unit $($Up to Department$)$, Province, 
%Zip Code, China}
%\newcommand{\ensecondinst}{Second Author's Working Unit $($Up to Department$)$, Province, 
%Zip Code, China}
%\newcommand{\entitle}{English Title}
%\newcommand{\enkeywords}{keyword 1; keyword 2; ......}
%\newcommand{\amsno}{62Nxx} % AMS Subject Claassification
%%
%% 中文摘要
%\newcommand{\cnabstract}{摘要内容.}
%% 英文摘要


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%        文章正文                                              %
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\title{\zihao{3}\bf{\cntitle}
}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% 作者姓名与单位：三种形式中选一种
% 后面英文摘要中的名字和单位同样处理
% ---------------------
% 第一种形式: 单一作者
% ---------------------
%\author{\zihao{4}\fangsong{\cnfirstauthor}\\[-1pt]
%{\zihao{-5}(\cnfirstinst)}}

% ---------------------
% 第二种形式: 同一单位 多个作者 -- 名字左右并列,
% ---------------------
%\author{\zihao{4}\fangsong{\cnfirstauthor\hy\hy\hy\cnsecondauthor}\\[-1pt]
%{\zihao{-5}(\cnfirstinst)}}

% ---------------------
% 第三种形式: 不同单位 多个作者 -- 名字与单位上下并列
% ---------------------
\author{\zihao{4}\fangsong{\cnfirstauthor}\\[-1pt]
{\zihao{-5}(\cnfirstinst)}\and
\zihao{4}\fangsong{\cnsecondauthor}\\[-1pt]
{\zihao{-5}(\cnsecondinst)}\and
\zihao{4}\fangsong{\cnthirdauthor}\\[-1pt]
{\zihao{-5}(\cnthirdinst)}\and
\zihao{4}\fangsong{\cnfourthauthor}\\[-1pt]
{\zihao{-5}(\cnfourthinst)}
}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\date{} % 这一行用来去掉默认的日期显示
\maketitle
\vspace{-6mm}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%  中文摘要
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{minipage}[c]{13.5cm}
\zihao{-5}
\textbf{摘~~~要:}\quad 本次实验我们小组首先在本地上安装并配置了伪分布式HBase与Hive环境，然后在上次完成的带词频属性的文档倒排算法任务基础上进行代码的修改，完成其在HBase上的插入与遍历任务以及在Hive上的导入与查询任务。\\
\\ \textbf{关键词:}\quad\cnkeywords
\end{minipage}

\hypersetup{CJKbookmarks=true}
\section{引 \hy \hy \hy言}

HBase是一个分布式的、面向列的开源数据库，HBase在Hadoop之上提供了类似于Bigtable的能力。HBase是Apache的Hadoop项目的子项目。HBase不同于一般的关系数据库，它是一个适合于非结构化数据存储的数据库。hive是基于Hadoop的一个数据仓库工具，可以将结构化的数据文件映射为一张数据库表，并提供简单的sql查询功能，可以将sql语句转换为MapReduce任务进行运行。在本次实验中，我们将上次实验的内容进行修改，使之能完成在HBase与Hive上的相关任务。\\

实验报告的第2节简要介绍了实验环境和完成情况。第3节中将详细介绍实验各个部分的设计。测试与运行的结果留在第4节中展示。在第5节中总结实验内容和团队合作。
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\newcommand{\authorsinfo}{陈越琦：121160005 完成选作任务并编写部分实验报告~~~~~刘威：131220085 完成必做任务前四部分并编写部分实验报告}
\newcommand{\authorsinfoo}{杨杰才：121160005 完成必做任务Hive部分并编写部分实验报告~~~~~周子博：121250229 }

\footnote[1]{\scriptsize{\authorsinfo}}\vspace{2em}
\footnote[0]{\scriptsize{\authorsinfoo}}\vspace{2em}
\newpage
\tableofcontents
\newpage

\section{实验环境与概述}
\noindent
本次实验的本地开发与测试环境如下：\\
\begin{table}[H]
\caption{开发测试环境}
\centering
\begin{tabular}{|c|c|}
\hline
软件 & 版本号\\
\hline
OS & ubuntu 15.04\\
\hline
kernel & 4.2.0-36-generic\\
\hline
JDK & 1.8.0\_66\\
\hline
Hadoop & 2.7.1\\
\hline
Hbase & 1.2.1\\
\hline
Hive & 1.2\\
\hline
\end{tabular}
\end{table}
本次实验依次完成了以下实验任务
\begin{enumerate}
\item 安装HBase和Hive
\item 在HBase中创建“Wuxia”表并修改第2次实验中的MapReduce程序， 在Reduce阶段将倒排索引的信息通过文件输出，同时把每个词语对应的“平均出现次数”信息写入到HBase的"Wuxia"表中
\item 将HBase中“Wuxia”表的表格内容保存在本地文件中。
\item 通过Hive Shell命令行创建表"Wuxia"，导入平均出现次数和相应词语并查询
\end{enumerate}

\subsection{HBase与Hive的安装与配置}
\subsubsection*{HBase的安装与配置}
\noindent
本部分，我们主要参考了网上的教程[2]来进行安装与配置，具体过程如下：\\
本次使用的Hadoop版本是2.7.1，通过官方文档查询得知与之较好匹配的Hase版本是1.2.1，下载安装包后，按照如下步骤进行安装与配置。
\begin{enumerate}
\item	首先解压安装包，并放到对应的目录
\item	与Hadoop安装一样，对/etc/profile文件进行修改，配置环境变量并使之生效
\item	修改配置文件：$<$HBASE\_HOME$>$/conf/hbase-site.xml 
\item	启动Hadoop和HDFS，并通过start-hbase.sh运行HBase，进行简单测试后，HBase正常运行，配置成功。
\item   要运行涉及HBase JAVA API的程序还需要将$<$HBASE\_HOME$>$/lib的路径加入到\\HADOOP\_HOME环境变量中。
\end{enumerate}
安装与配置成功后没，使用jps命令查看当前运行的所有java进程如下：
\begin{center}
\includegraphics[width = 14cm]{22.png}
\caption{jps命令查看java进程}
可以看到，hadoop与hbase均成功启动。
\end{center}
\subsubsection*{Hive的安装与配置}
\noindent
本部分，我们主要参考了网上的教程[3]来进行安装与配置，具体过程如下：\\
本次使用的Hadoop版本是2.7.1，通过官方文档查询得知与之较好匹配的Hive版本是1.2.1，下载安装包后，按照如下步骤进行安装。
\begin{enumerate}
\item	首先用apt-get命令按照了SQL
\item	与Hadoop安装一样，对/etc/profile文件进行修改，配置环境变量并使之生效
\item	复制hive /conf/hive-default.xml为hive-site.xml，并修改hive-site.xml中的内容
\item	下载mysql-connector-java-5.1.27-bin.jar文件，并放到\$HIVE\_HOME/lib目录下
\item	启动Hadoop，并运行Hive，进行简单测试后，Hive正常运行，配置成功。
\end{enumerate}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{实验设计思路}
\subsection{在Reduce阶段将倒排索引结果的部分内容插入到HBase中}
\noindent 
为实现这个任务，我们选择使用HBase的JAVA API来实现对HBase数据库进行操作，将词语与平均出现次数插入到'Wuxia'表中。而除平均出现次数之外的倒排索引信息仍输出到HDFS中的输出文件中。在本任务中，我使用词语的名字作为HBase每一行的rowKey。完成本任务的具体过程如下：\\
~\\首先在HBase中通过shell命令行建立"Wuxia"表：\\
\textbf{hbase(main):xxx:x$>$create 'Wuxia', 'content'}\\

\noindent 
在实验2的基础上对Reducer类进行部分代码修改，从而实现将倒排索引结果的词语名与平均出现次数插入到HBase中的"Wuxia"表。为实现这一目标，我们在每个Reduce任务中将每个词语的平均出现次数和词语构造为一个Put类的实例，将这个实例插入到全局Put类型的List中，关键语句如下：

\textbf{static List$<$Put$>$ putList = new ArrayList$<$Put$>$();} //全局变量

\textbf{...}
\noindent
//在reduce函数中，插入以下语句构造相应的Put变量：

\textbf{Put put = new Put(Bytes.toBytes(CurrentItem.toString()));}

\textbf{put.add(Bytes.toBytes(new String("content")), Bytes.toBytes(new String("frequency")), Bytes.toBytes(Double.toString(fre)));}

\textbf{putList.add(put);}
\\
\noindent
最后，在cleanup函数中，建立一个HBase Job，并通过zookeeper建立连接，调用HTable类的put函数将List中的内容插入到HBase中，关键语句如下：

//在HBase类的构造函数中，建立HBase Job，并通过zookeeper建立连接

\textbf{conf = HBaseConfiguration.create();}
\textbf{conf.set("hbase.zookeeper.property.clientPort", "2181");}

\textbf{...}

//在cleanup阶段将结果一次性输入到HBase中

\textbf{ HBase hbase = new HBase();}
\textbf{hbase.addDatas("Wuxia",putList);}

//addDatas函数是将PutList中的内容插入到'Wuxia'表中的函数，通过HTable类的put函数将Put实例依次插入到HBase的'Wuxia'表中。\\
同时，保持实验二其他部分不变即可完成文件输出与HBase插入两部分过程。具体的实验过程与结果的截图将在第四节中展示。
\subsection{把HBase中"Wuxia"表保存到本地}
\noindent
本任务中，我们编写了ReadHBase.java文件来实现这一任务。我们同样通过调用HBase的JAVA~API来实现对'Wuxia'表的遍历，然后将相应的内容按照~词语 TAB 平均出现次数~的格式输出到输出文件output中。\\
~\\为完成这个任务，我们使用io.File类来进行文件的输出操作。实验的大体设计分为以下三部分：\\
(1)建立一个HBase Job，进行设置并通过zookeeper建立连接，关键代码如下：\\
	\textbf{conf = HBaseConfiguration.create();}\\
	\textbf{conf.set("hbase.zookeeper.quo.rum", "localhost");}\\
	\textbf{conf.set("hbase.zookeeper.property.clientPort", "2181");}\\
(2)建立输出文件并通过HTable提供的getScanner方法来进行批量查询，创建Scan变量时使用默认空参数从而实现对整个表的遍历。本部分关键代码如下：\\
 	\textbf{File writename = new File("output"); }\\
    \textbf{writename.createNewFile();}\\  
    \textbf{Table table = new HTable(conf,tableName);}\\
	\textbf{Scan s = new Scan();}\\
	\textbf{ResultScanner ss = table.getScanner(s);}\\
(3)对批量查询结果进行遍历，将相应的词语名rowKey与平均出现次数frequency输出到output文件。本部分关键代码如下：\\
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%排版仍需优化
\textbf{for(Result r : ss)}\\
\textbf{\{}\\
    ~~~~for(KeyValue kv : r.raw())\\
    ~~~~\{\\
    ~~~~~~~~out.write(...);\\
    ~~~~\}\\
    ~~~~out.flush();\\
\textbf{\}}\\
\subsection{将数据导入到Hive中并查询}
\noindent
本部分任务，我们通过Hive Shell来完成。首先创建Table~Wuxia，然后将上一任务中的输出文件output导入到Wuxia中，最后通过相应的查询指令进行查询。具体的过程在第四节中会详细叙述。

\section{实验测试与运行结果}

\subsection{在Reduce阶段将倒排索引的结果输出到HBase中}
\noindent
本部分的测试与运行过程如下：\\
(1)~将集群HDFS中的测试文件复制到本地，然后使用scp拷贝到我们的机器中。\\
(2)~将修改后的代码编译得到.class文件并打包为InvertedIndex.jar包。\\
(3)~在HBase中创建表'Wuxia'，如下图：\\
\begin{center}
\includegraphics[width = 14cm]{23.png}
\caption{建表与查看}
\end{center}
(4)~使用命令hadoop jar InvertedIndex.jar InvertedIndex Lab3 Lab3out~运行这一任务。\\
~\\本任务的运行结果如下：
\begin{center}
\includegraphics[width = 14cm]{19.png}
\caption{HDFS中的输出倒排索引信息}
\end{center}
可以看到，倒排索引中的信息已不再输出平均出现次数。\\
~\\使用scan 'Wuxia' 命令查看HBase中'Wuxia'表的插入结果如下（rowKey为词语的UTF-8编码，frequency为该词语的平均出现次数）：\\
\begin{center}
\includegraphics[width = 14cm]{20.png}
\caption{'Wuxia'表中信息}
\end{center}
可以看到总共134881条记录已经全部成功插入到'Wuxia'表中。
\subsection{把HBase中"Wuxia"表保存到本地}
\noindent
本任务的测试与运行过程如下：\\
(1)~编译ReadHBase.java并打包得到ReadHBase.jar。\\
(2)~使用hadoop jar ReadHBase.jar ReadHBase 命令来执行这一任务。\\
(3)~执行结束后，在当前文件夹下即可看到输出文件output。\\
~\\得到的输出文件output的部分内容的截图如下：
\begin{center}
\includegraphics[width = 14cm]{21.png}
\caption{'Wuxia'表保存到本地的文件output}
\end{center}
\subsection{将数据导入到Hive中并查询}
\noindent
本部分全部在Hive Shell下完成，首先建表，然后将上面的输出文件output导入表中，最后查询。相应的命令与查询结果见下面的截图：
\begin{center}
\includegraphics[width = 15cm]{1.png}
\caption{创建表}
\end{center}

\begin{center}
\includegraphics[width = 15cm]{2.png}
\caption{将数据导入Hive}
\end{center}

\begin{center}
\includegraphics[width = 10cm]{3.png}
\includegraphics[width = 10cm]{4.png}
\includegraphics[width = 10cm]{5.png}
\includegraphics[width = 10cm]{6.png}
\includegraphics[width = 10cm]{7.png}
\includegraphics[width = 10cm]{8.png}
\includegraphics[width = 10cm]{9.png}
\includegraphics[width = 10cm]{10.png}
\includegraphics[width = 10cm]{11.png}
\caption{查询平均出现次数大于300的词语}
\end{center}

\begin{center}
\includegraphics[width = 10cm]{12.png}
\includegraphics[width = 10cm]{13.png}
\includegraphics[width = 10cm]{14.png}
\includegraphics[width = 10cm]{15.png}
\includegraphics[width = 10cm]{16.png}
\includegraphics[width = 10cm]{17.png}
\includegraphics[width = 10cm]{18.png}
\caption{平均次数前100的词语}
\end{center}

\section{实验总结}

\vspace{6mm}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%  参考文献
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\newpage
\zihao{-5}
\begin{thebibliography}{99}
%\setlength{\parskip}{0pt}  %段落之间的竖直距离
\addtolength{\itemsep}{-0.8 em} % 缩小参考文献间的垂直间距
  \bibitem{Book} 黄宜华. {\kaishu 深入理解大数据~大数据处理与编程实践}[M]. 北京: 机械工业出版社, 2014.7.
  \bibitem{Book} HBase安装配置之伪分布式模式-pdw2009的专栏-博客频道-CSDN.NET
\\http://blog.csdn.net/pdw2009/article/details/21261417
  \bibitem{Book} hadoop2.2 学习5 ubuntu在伪分布式环境下安装hive-幸运声的日志-网易博客\\
http://blog.163.com/gibby\_l/blog/static/83003161201402410204518/
\end{thebibliography}
\clearpage
\end{document}
